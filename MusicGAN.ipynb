{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYqTHRiqAQGx",
        "colab_type": "text"
      },
      "source": [
        "# Music Generation GAN\n",
        "Approach used for reference: https://towardsdatascience.com/generating-pokemon-inspired-music-from-neural-networks-bc240014132"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P9XLdrFaz-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import glob\n",
        "from __future__ import print_function, division\n",
        "import random\n",
        "import pickle\n",
        "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from google.colab import files, drive\n",
        "from music21 import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEFTP6B_a3vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Helper functions for processing MIDI files\n",
        "\"\"\"\n",
        "def get_notes():\n",
        "    \"\"\"\n",
        "    Get all the notes and chords from the midi files\n",
        "    \"\"\"\n",
        "    if os.path.exists('midi.pickle'):\n",
        "        notes = pickle.load(open(\"midi.pickle\",\"rb\"))\n",
        "    else:\n",
        "        notes = []\n",
        "\n",
        "        for file in glob.glob(\"midi_data/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try: # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse() \n",
        "            except: # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "                \n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        with open('midi.pickle', 'wb') as filepath:\n",
        "                pickle.dump(notes, filepath)\n",
        "    return notes\n",
        "\n",
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # Create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    \n",
        "    # Normalize input between -1 and 1\n",
        "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)\n",
        "\n",
        "def generate_notes(model, network_input, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "    \n",
        "    # Get pitch names and store in a dictionary\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "        \n",
        "        pattern = numpy.append(pattern,index)\n",
        "        #pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output\n",
        "  \n",
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item[0]\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OK-ryjk9wsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Functions to initialize networks\n",
        "\"\"\"\n",
        "def build_discriminator(seq_shape):\n",
        "    model = Sequential()\n",
        "    model.add(CuDNNLSTM(512, input_shape=seq_shape, return_sequences=True))\n",
        "    model.add(Bidirectional(CuDNNLSTM(512)))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    seq = Input(shape=seq_shape)\n",
        "    validity = model(seq)\n",
        "\n",
        "    return Model(seq, validity)\n",
        "\n",
        "def build_generator(seq_shape, latent_dim):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
        "    model.add(Reshape(seq_shape))\n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    seq = model(noise)\n",
        "\n",
        "    return Model(noise, seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbMMVyuE9woq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Functions for performing and plotting training\n",
        "\"\"\"\n",
        "def train(discriminator, generator, combined, notes, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "    # Load and convert the data\n",
        "    n_vocab = len(set(notes))\n",
        "    X_train, y_train = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    # Training the model\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Training the discriminator\n",
        "        # Select a random batch of note sequences\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_seqs = X_train[idx]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "        # Generate a batch of new note sequences\n",
        "        gen_seqs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_seqs, real)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_seqs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "        #  Training the Generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "        # Train the generator (to have the discriminator label samples as real)\n",
        "        g_loss = combined.train_on_batch(noise, real)\n",
        "\n",
        "        # Print the progress and save into loss lists\n",
        "        if epoch % sample_interval == 0:\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            disc_loss.append(d_loss[0])\n",
        "            gen_loss.append(g_loss)\n",
        "\n",
        "def plot_loss(disc_loss, gen_loss):\n",
        "    plt.plot(disc_loss, c='red')\n",
        "    plt.plot(gen_loss, c='blue')\n",
        "    plt.title(\"GAN Loss per Epoch\")\n",
        "    plt.legend(['Discriminator', 'Generator'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=False)\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxdO2wrm9wk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "37eead93-1bdf-41ec-c45f-656561832c49"
      },
      "source": [
        "# Set sequence length to 100 notes\n",
        "seq_length = 100\n",
        "seq_shape = (seq_length, 1)\n",
        "latent_dim = 1000\n",
        "disc_loss = []\n",
        "gen_loss =[]\n",
        "\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(seq_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(seq_shape, latent_dim)\n",
        "\n",
        "# The generator takes noise as input and generates note sequences\n",
        "z = Input(shape=(latent_dim,))\n",
        "generated_seq = generator(z)\n",
        "\n",
        "# For the combined model we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated images as input and determines validity\n",
        "validity = discriminator(generated_seq)\n",
        "\n",
        "# The combined model  (stacked generator and discriminator)\n",
        "# Trains the generator to fool the discriminator\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 100, 512)          1054720   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 1024)              4202496   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,913,601\n",
            "Trainable params: 5,913,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 256)               256256    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               102500    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 100, 1)            0         \n",
            "=================================================================\n",
            "Total params: 1,022,820\n",
            "Trainable params: 1,019,236\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGS_MwHGa30T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "notes = get_notes()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks_e4ZfnoqHl",
        "colab_type": "code",
        "outputId": "dfb9165f-f230-4016-faac-9e5cf07c0e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(discriminator, generator, combined, notes, epochs=5000, batch_size=32, sample_interval=1)\n",
        "plot_loss(disc_loss, gen_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.696695, acc.: 45.31%] [G loss: 0.689703]\n",
            "1 [D loss: 0.668604, acc.: 76.56%] [G loss: 0.687677]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2 [D loss: 0.630965, acc.: 73.44%] [G loss: 0.691364]\n",
            "3 [D loss: 0.546545, acc.: 82.81%] [G loss: 0.711633]\n",
            "4 [D loss: 0.421584, acc.: 78.12%] [G loss: 0.846053]\n",
            "5 [D loss: 0.297144, acc.: 90.62%] [G loss: 1.211569]\n",
            "6 [D loss: 0.142243, acc.: 98.44%] [G loss: 3.936227]\n",
            "7 [D loss: 0.002166, acc.: 100.00%] [G loss: 9.128134]\n",
            "8 [D loss: 0.007691, acc.: 100.00%] [G loss: 11.909663]\n",
            "9 [D loss: 0.000951, acc.: 100.00%] [G loss: 12.731110]\n",
            "10 [D loss: 0.235037, acc.: 96.88%] [G loss: 2.548332]\n",
            "11 [D loss: 0.140999, acc.: 98.44%] [G loss: 2.792893]\n",
            "12 [D loss: 0.147357, acc.: 95.31%] [G loss: 2.344263]\n",
            "13 [D loss: 0.141752, acc.: 96.88%] [G loss: 3.200874]\n",
            "14 [D loss: 0.089833, acc.: 96.88%] [G loss: 5.909007]\n",
            "15 [D loss: 0.141520, acc.: 98.44%] [G loss: 6.441468]\n",
            "16 [D loss: 0.100843, acc.: 96.88%] [G loss: 5.054346]\n",
            "17 [D loss: 0.083480, acc.: 96.88%] [G loss: 6.381463]\n",
            "18 [D loss: 0.095199, acc.: 96.88%] [G loss: 5.358906]\n",
            "19 [D loss: 0.209385, acc.: 90.62%] [G loss: 4.312189]\n",
            "20 [D loss: 0.099972, acc.: 98.44%] [G loss: 4.290300]\n",
            "21 [D loss: 0.292586, acc.: 87.50%] [G loss: 4.733485]\n",
            "22 [D loss: 0.171914, acc.: 95.31%] [G loss: 3.932982]\n",
            "23 [D loss: 0.201832, acc.: 90.62%] [G loss: 4.132977]\n",
            "24 [D loss: 0.151856, acc.: 95.31%] [G loss: 4.058130]\n",
            "25 [D loss: 0.208564, acc.: 87.50%] [G loss: 4.020058]\n",
            "26 [D loss: 0.165278, acc.: 92.19%] [G loss: 4.315030]\n",
            "27 [D loss: 0.062090, acc.: 98.44%] [G loss: 5.653381]\n",
            "28 [D loss: 0.499894, acc.: 85.94%] [G loss: 3.181675]\n",
            "29 [D loss: 0.291167, acc.: 95.31%] [G loss: 2.712367]\n",
            "30 [D loss: 0.330143, acc.: 90.62%] [G loss: 3.168332]\n",
            "31 [D loss: 0.249813, acc.: 95.31%] [G loss: 4.396722]\n",
            "32 [D loss: 0.370980, acc.: 89.06%] [G loss: 4.185214]\n",
            "33 [D loss: 0.242505, acc.: 92.19%] [G loss: 3.440242]\n",
            "34 [D loss: 0.271456, acc.: 90.62%] [G loss: 3.613347]\n",
            "35 [D loss: 0.259617, acc.: 85.94%] [G loss: 3.247585]\n",
            "36 [D loss: 0.279964, acc.: 90.62%] [G loss: 3.585974]\n",
            "37 [D loss: 0.240614, acc.: 90.62%] [G loss: 3.345114]\n",
            "38 [D loss: 0.121573, acc.: 93.75%] [G loss: 4.254866]\n",
            "39 [D loss: 0.220871, acc.: 92.19%] [G loss: 4.075439]\n",
            "40 [D loss: 0.203963, acc.: 92.19%] [G loss: 3.480292]\n",
            "41 [D loss: 0.264367, acc.: 90.62%] [G loss: 3.358156]\n",
            "42 [D loss: 0.233372, acc.: 89.06%] [G loss: 3.706346]\n",
            "43 [D loss: 0.289485, acc.: 85.94%] [G loss: 3.775117]\n",
            "44 [D loss: 0.301621, acc.: 84.38%] [G loss: 2.808489]\n",
            "45 [D loss: 0.306036, acc.: 84.38%] [G loss: 3.027232]\n",
            "46 [D loss: 0.222790, acc.: 90.62%] [G loss: 3.176851]\n",
            "47 [D loss: 0.111374, acc.: 95.31%] [G loss: 3.759203]\n",
            "48 [D loss: 0.328226, acc.: 87.50%] [G loss: 4.264867]\n",
            "49 [D loss: 0.135658, acc.: 96.88%] [G loss: 4.666701]\n",
            "50 [D loss: 0.308953, acc.: 89.06%] [G loss: 3.155695]\n",
            "51 [D loss: 0.157569, acc.: 95.31%] [G loss: 3.889422]\n",
            "52 [D loss: 0.319120, acc.: 89.06%] [G loss: 2.769181]\n",
            "53 [D loss: 0.114394, acc.: 98.44%] [G loss: 3.467358]\n",
            "54 [D loss: 0.245481, acc.: 85.94%] [G loss: 4.076035]\n",
            "55 [D loss: 0.109635, acc.: 93.75%] [G loss: 4.703959]\n",
            "56 [D loss: 0.095061, acc.: 98.44%] [G loss: 6.195454]\n",
            "57 [D loss: 0.290568, acc.: 90.62%] [G loss: 3.861240]\n",
            "58 [D loss: 0.101903, acc.: 96.88%] [G loss: 3.788749]\n",
            "59 [D loss: 0.164946, acc.: 93.75%] [G loss: 4.341498]\n",
            "60 [D loss: 0.069012, acc.: 98.44%] [G loss: 5.159225]\n",
            "61 [D loss: 0.154466, acc.: 96.88%] [G loss: 4.669841]\n",
            "62 [D loss: 0.234760, acc.: 93.75%] [G loss: 4.192533]\n",
            "63 [D loss: 0.235180, acc.: 89.06%] [G loss: 4.109934]\n",
            "64 [D loss: 0.208330, acc.: 87.50%] [G loss: 4.311921]\n",
            "65 [D loss: 0.186227, acc.: 96.88%] [G loss: 4.624017]\n",
            "66 [D loss: 0.189222, acc.: 92.19%] [G loss: 5.269263]\n",
            "67 [D loss: 0.170406, acc.: 90.62%] [G loss: 5.575547]\n",
            "68 [D loss: 0.340145, acc.: 85.94%] [G loss: 3.225957]\n",
            "69 [D loss: 0.219742, acc.: 93.75%] [G loss: 2.880410]\n",
            "70 [D loss: 0.286788, acc.: 90.62%] [G loss: 3.176200]\n",
            "71 [D loss: 0.212496, acc.: 93.75%] [G loss: 3.974093]\n",
            "72 [D loss: 0.142640, acc.: 95.31%] [G loss: 5.453626]\n",
            "73 [D loss: 0.273101, acc.: 92.19%] [G loss: 3.271312]\n",
            "74 [D loss: 0.189818, acc.: 90.62%] [G loss: 3.513006]\n",
            "75 [D loss: 0.189395, acc.: 93.75%] [G loss: 4.817850]\n",
            "76 [D loss: 0.073230, acc.: 98.44%] [G loss: 5.751447]\n",
            "77 [D loss: 0.194415, acc.: 92.19%] [G loss: 6.917565]\n",
            "78 [D loss: 0.099753, acc.: 95.31%] [G loss: 8.810005]\n",
            "79 [D loss: 0.087019, acc.: 96.88%] [G loss: 10.132690]\n",
            "80 [D loss: 0.479267, acc.: 85.94%] [G loss: 2.158693]\n",
            "81 [D loss: 0.467209, acc.: 79.69%] [G loss: 1.932274]\n",
            "82 [D loss: 0.394067, acc.: 82.81%] [G loss: 1.978030]\n",
            "83 [D loss: 0.362100, acc.: 85.94%] [G loss: 2.076607]\n",
            "84 [D loss: 0.269337, acc.: 92.19%] [G loss: 2.718575]\n",
            "85 [D loss: 0.271453, acc.: 89.06%] [G loss: 3.509791]\n",
            "86 [D loss: 0.271120, acc.: 90.62%] [G loss: 4.245824]\n",
            "87 [D loss: 0.186792, acc.: 93.75%] [G loss: 4.054444]\n",
            "88 [D loss: 0.214674, acc.: 93.75%] [G loss: 3.922885]\n",
            "89 [D loss: 0.236338, acc.: 92.19%] [G loss: 3.550535]\n",
            "90 [D loss: 0.281943, acc.: 87.50%] [G loss: 3.899301]\n",
            "91 [D loss: 0.396095, acc.: 82.81%] [G loss: 3.613122]\n",
            "92 [D loss: 0.233652, acc.: 87.50%] [G loss: 3.457515]\n",
            "93 [D loss: 0.352746, acc.: 81.25%] [G loss: 2.946743]\n",
            "94 [D loss: 0.330740, acc.: 89.06%] [G loss: 2.736021]\n",
            "95 [D loss: 0.327080, acc.: 89.06%] [G loss: 2.869841]\n",
            "96 [D loss: 0.177812, acc.: 95.31%] [G loss: 4.582571]\n",
            "97 [D loss: 0.384749, acc.: 90.62%] [G loss: 2.302239]\n",
            "98 [D loss: 0.247575, acc.: 89.06%] [G loss: 2.516731]\n",
            "99 [D loss: 0.234113, acc.: 90.62%] [G loss: 2.562012]\n",
            "100 [D loss: 0.261791, acc.: 90.62%] [G loss: 2.776565]\n",
            "101 [D loss: 0.366199, acc.: 81.25%] [G loss: 2.548021]\n",
            "102 [D loss: 0.168784, acc.: 92.19%] [G loss: 3.365224]\n",
            "103 [D loss: 0.213889, acc.: 89.06%] [G loss: 3.515203]\n",
            "104 [D loss: 0.254945, acc.: 89.06%] [G loss: 3.566744]\n",
            "105 [D loss: 0.239054, acc.: 85.94%] [G loss: 3.837652]\n",
            "106 [D loss: 0.141479, acc.: 96.88%] [G loss: 4.598569]\n",
            "107 [D loss: 0.291873, acc.: 87.50%] [G loss: 4.762460]\n",
            "108 [D loss: 0.430037, acc.: 82.81%] [G loss: 2.121694]\n",
            "109 [D loss: 0.407006, acc.: 79.69%] [G loss: 1.912290]\n",
            "110 [D loss: 0.325352, acc.: 82.81%] [G loss: 2.169401]\n",
            "111 [D loss: 0.227977, acc.: 93.75%] [G loss: 3.163201]\n",
            "112 [D loss: 0.415195, acc.: 85.94%] [G loss: 2.473895]\n",
            "113 [D loss: 0.241272, acc.: 90.62%] [G loss: 2.427617]\n",
            "114 [D loss: 0.185542, acc.: 92.19%] [G loss: 2.875442]\n",
            "115 [D loss: 0.272042, acc.: 82.81%] [G loss: 3.405754]\n",
            "116 [D loss: 0.351932, acc.: 84.38%] [G loss: 2.718153]\n",
            "117 [D loss: 0.284149, acc.: 87.50%] [G loss: 2.837169]\n",
            "118 [D loss: 0.360872, acc.: 87.50%] [G loss: 2.228674]\n",
            "119 [D loss: 0.289528, acc.: 87.50%] [G loss: 2.466216]\n",
            "120 [D loss: 0.317068, acc.: 84.38%] [G loss: 3.329338]\n",
            "121 [D loss: 0.218848, acc.: 92.19%] [G loss: 3.729987]\n",
            "122 [D loss: 0.231794, acc.: 89.06%] [G loss: 3.719571]\n",
            "123 [D loss: 0.293009, acc.: 87.50%] [G loss: 3.439303]\n",
            "124 [D loss: 0.243466, acc.: 90.62%] [G loss: 3.460501]\n",
            "125 [D loss: 0.116206, acc.: 96.88%] [G loss: 5.680163]\n",
            "126 [D loss: 0.455181, acc.: 85.94%] [G loss: 2.389919]\n",
            "127 [D loss: 0.210164, acc.: 90.62%] [G loss: 2.491588]\n",
            "128 [D loss: 0.317658, acc.: 87.50%] [G loss: 2.354792]\n",
            "129 [D loss: 0.262087, acc.: 89.06%] [G loss: 2.789325]\n",
            "130 [D loss: 0.119867, acc.: 95.31%] [G loss: 3.799516]\n",
            "131 [D loss: 0.312717, acc.: 87.50%] [G loss: 3.355841]\n",
            "132 [D loss: 0.300758, acc.: 84.38%] [G loss: 3.156410]\n",
            "133 [D loss: 0.484406, acc.: 79.69%] [G loss: 2.224205]\n",
            "134 [D loss: 0.302556, acc.: 89.06%] [G loss: 2.922508]\n",
            "135 [D loss: 0.306829, acc.: 87.50%] [G loss: 3.970758]\n",
            "136 [D loss: 0.312597, acc.: 85.94%] [G loss: 3.559494]\n",
            "137 [D loss: 0.279577, acc.: 90.62%] [G loss: 3.065988]\n",
            "138 [D loss: 0.434344, acc.: 82.81%] [G loss: 2.399089]\n",
            "139 [D loss: 0.339301, acc.: 85.94%] [G loss: 2.361960]\n",
            "140 [D loss: 0.268210, acc.: 84.38%] [G loss: 3.058881]\n",
            "141 [D loss: 0.228851, acc.: 92.19%] [G loss: 4.575991]\n",
            "142 [D loss: 0.759026, acc.: 71.88%] [G loss: 1.506405]\n",
            "143 [D loss: 0.459620, acc.: 81.25%] [G loss: 1.426527]\n",
            "144 [D loss: 0.562654, acc.: 73.44%] [G loss: 1.479445]\n",
            "145 [D loss: 0.535171, acc.: 73.44%] [G loss: 1.701468]\n",
            "146 [D loss: 0.473653, acc.: 76.56%] [G loss: 2.314415]\n",
            "147 [D loss: 0.310572, acc.: 82.81%] [G loss: 3.471846]\n",
            "148 [D loss: 0.624500, acc.: 79.69%] [G loss: 1.662146]\n",
            "149 [D loss: 0.471382, acc.: 79.69%] [G loss: 1.830905]\n",
            "150 [D loss: 0.543556, acc.: 70.31%] [G loss: 1.764215]\n",
            "151 [D loss: 0.598588, acc.: 70.31%] [G loss: 1.413498]\n",
            "152 [D loss: 0.490571, acc.: 75.00%] [G loss: 1.830675]\n",
            "153 [D loss: 0.600884, acc.: 70.31%] [G loss: 1.618560]\n",
            "154 [D loss: 0.534707, acc.: 71.88%] [G loss: 1.842214]\n",
            "155 [D loss: 0.484448, acc.: 79.69%] [G loss: 2.557452]\n",
            "156 [D loss: 0.585158, acc.: 68.75%] [G loss: 1.709022]\n",
            "157 [D loss: 0.374891, acc.: 81.25%] [G loss: 2.158433]\n",
            "158 [D loss: 0.591309, acc.: 73.44%] [G loss: 1.191507]\n",
            "159 [D loss: 0.535614, acc.: 73.44%] [G loss: 1.290328]\n",
            "160 [D loss: 0.582390, acc.: 70.31%] [G loss: 1.559661]\n",
            "161 [D loss: 0.588636, acc.: 60.94%] [G loss: 1.610889]\n",
            "162 [D loss: 0.453660, acc.: 75.00%] [G loss: 2.195081]\n",
            "163 [D loss: 0.604153, acc.: 65.62%] [G loss: 1.742662]\n",
            "164 [D loss: 0.553311, acc.: 73.44%] [G loss: 1.748275]\n",
            "165 [D loss: 0.564849, acc.: 70.31%] [G loss: 1.941679]\n",
            "166 [D loss: 0.582859, acc.: 65.62%] [G loss: 2.063879]\n",
            "167 [D loss: 0.488466, acc.: 82.81%] [G loss: 2.055679]\n",
            "168 [D loss: 0.474745, acc.: 79.69%] [G loss: 2.629137]\n",
            "169 [D loss: 0.391144, acc.: 79.69%] [G loss: 3.026454]\n",
            "170 [D loss: 0.798490, acc.: 67.19%] [G loss: 1.914337]\n",
            "171 [D loss: 0.511491, acc.: 73.44%] [G loss: 1.605622]\n",
            "172 [D loss: 0.552802, acc.: 65.62%] [G loss: 1.907512]\n",
            "173 [D loss: 0.562278, acc.: 68.75%] [G loss: 1.979989]\n",
            "174 [D loss: 0.597205, acc.: 67.19%] [G loss: 1.675015]\n",
            "175 [D loss: 0.558044, acc.: 68.75%] [G loss: 1.837282]\n",
            "176 [D loss: 0.518249, acc.: 70.31%] [G loss: 2.201547]\n",
            "177 [D loss: 0.607228, acc.: 70.31%] [G loss: 1.979920]\n",
            "178 [D loss: 0.615840, acc.: 64.06%] [G loss: 1.450396]\n",
            "179 [D loss: 0.500153, acc.: 75.00%] [G loss: 1.845013]\n",
            "180 [D loss: 0.497772, acc.: 73.44%] [G loss: 2.197386]\n",
            "181 [D loss: 0.597940, acc.: 65.62%] [G loss: 2.643372]\n",
            "182 [D loss: 0.598875, acc.: 64.06%] [G loss: 2.236042]\n",
            "183 [D loss: 0.414662, acc.: 81.25%] [G loss: 2.658344]\n",
            "184 [D loss: 0.482888, acc.: 76.56%] [G loss: 2.903628]\n",
            "185 [D loss: 0.624875, acc.: 68.75%] [G loss: 1.610071]\n",
            "186 [D loss: 0.551708, acc.: 73.44%] [G loss: 1.639604]\n",
            "187 [D loss: 0.534595, acc.: 73.44%] [G loss: 1.799214]\n",
            "188 [D loss: 0.589574, acc.: 67.19%] [G loss: 1.605089]\n",
            "189 [D loss: 0.530657, acc.: 71.88%] [G loss: 1.910947]\n",
            "190 [D loss: 0.538925, acc.: 70.31%] [G loss: 1.868070]\n",
            "191 [D loss: 0.519903, acc.: 73.44%] [G loss: 1.889169]\n",
            "192 [D loss: 0.622258, acc.: 67.19%] [G loss: 1.251234]\n",
            "193 [D loss: 0.554277, acc.: 75.00%] [G loss: 1.583706]\n",
            "194 [D loss: 0.444421, acc.: 79.69%] [G loss: 2.286946]\n",
            "195 [D loss: 0.418206, acc.: 82.81%] [G loss: 2.415240]\n",
            "196 [D loss: 0.674623, acc.: 70.31%] [G loss: 1.072270]\n",
            "197 [D loss: 0.563291, acc.: 75.00%] [G loss: 1.379983]\n",
            "198 [D loss: 0.522094, acc.: 70.31%] [G loss: 1.528791]\n",
            "199 [D loss: 0.541661, acc.: 68.75%] [G loss: 1.708537]\n",
            "200 [D loss: 0.576068, acc.: 62.50%] [G loss: 1.505366]\n",
            "201 [D loss: 0.576096, acc.: 70.31%] [G loss: 1.567374]\n",
            "202 [D loss: 0.384850, acc.: 82.81%] [G loss: 1.993040]\n",
            "203 [D loss: 0.639416, acc.: 73.44%] [G loss: 1.092333]\n",
            "204 [D loss: 0.585459, acc.: 67.19%] [G loss: 1.155478]\n",
            "205 [D loss: 0.575066, acc.: 70.31%] [G loss: 1.159645]\n",
            "206 [D loss: 0.535989, acc.: 68.75%] [G loss: 1.288707]\n",
            "207 [D loss: 0.508027, acc.: 75.00%] [G loss: 1.492904]\n",
            "208 [D loss: 0.658663, acc.: 59.38%] [G loss: 1.165018]\n",
            "209 [D loss: 0.523293, acc.: 79.69%] [G loss: 1.326077]\n",
            "210 [D loss: 0.564933, acc.: 71.88%] [G loss: 1.339098]\n",
            "211 [D loss: 0.509647, acc.: 81.25%] [G loss: 1.671081]\n",
            "212 [D loss: 0.580051, acc.: 71.88%] [G loss: 1.601394]\n",
            "213 [D loss: 0.441809, acc.: 84.38%] [G loss: 1.923314]\n",
            "214 [D loss: 0.541243, acc.: 71.88%] [G loss: 1.577940]\n",
            "215 [D loss: 0.552825, acc.: 76.56%] [G loss: 1.449104]\n",
            "216 [D loss: 0.569303, acc.: 76.56%] [G loss: 1.404976]\n",
            "217 [D loss: 0.628973, acc.: 64.06%] [G loss: 1.188276]\n",
            "218 [D loss: 0.695055, acc.: 53.12%] [G loss: 1.043096]\n",
            "219 [D loss: 0.606946, acc.: 65.62%] [G loss: 1.308707]\n",
            "220 [D loss: 0.605206, acc.: 70.31%] [G loss: 1.253607]\n",
            "221 [D loss: 0.513270, acc.: 76.56%] [G loss: 1.599380]\n",
            "222 [D loss: 0.591423, acc.: 68.75%] [G loss: 1.214110]\n",
            "223 [D loss: 0.539748, acc.: 75.00%] [G loss: 1.263325]\n",
            "224 [D loss: 0.574875, acc.: 73.44%] [G loss: 1.350032]\n",
            "225 [D loss: 0.673912, acc.: 60.94%] [G loss: 1.062424]\n",
            "226 [D loss: 0.659375, acc.: 64.06%] [G loss: 1.015325]\n",
            "227 [D loss: 0.749061, acc.: 54.69%] [G loss: 1.095537]\n",
            "228 [D loss: 0.583915, acc.: 70.31%] [G loss: 1.086896]\n",
            "229 [D loss: 0.676758, acc.: 64.06%] [G loss: 0.986069]\n",
            "230 [D loss: 0.574153, acc.: 71.88%] [G loss: 1.114677]\n",
            "231 [D loss: 0.611290, acc.: 67.19%] [G loss: 1.184964]\n",
            "232 [D loss: 0.551605, acc.: 76.56%] [G loss: 1.449768]\n",
            "233 [D loss: 0.648697, acc.: 65.62%] [G loss: 1.109444]\n",
            "234 [D loss: 0.600840, acc.: 68.75%] [G loss: 1.085208]\n",
            "235 [D loss: 0.530018, acc.: 70.31%] [G loss: 1.189905]\n",
            "236 [D loss: 0.633086, acc.: 64.06%] [G loss: 1.271895]\n",
            "237 [D loss: 0.648043, acc.: 71.88%] [G loss: 0.966271]\n",
            "238 [D loss: 0.592112, acc.: 67.19%] [G loss: 0.987352]\n",
            "239 [D loss: 0.692490, acc.: 59.38%] [G loss: 0.911618]\n",
            "240 [D loss: 0.676878, acc.: 56.25%] [G loss: 0.937772]\n",
            "241 [D loss: 0.611808, acc.: 70.31%] [G loss: 0.967734]\n",
            "242 [D loss: 0.660968, acc.: 64.06%] [G loss: 0.966921]\n",
            "243 [D loss: 0.669169, acc.: 64.06%] [G loss: 1.001833]\n",
            "244 [D loss: 0.661063, acc.: 59.38%] [G loss: 0.996207]\n",
            "245 [D loss: 0.692611, acc.: 62.50%] [G loss: 0.935049]\n",
            "246 [D loss: 0.652437, acc.: 64.06%] [G loss: 0.906438]\n",
            "247 [D loss: 0.631265, acc.: 65.62%] [G loss: 0.946856]\n",
            "248 [D loss: 0.606328, acc.: 65.62%] [G loss: 1.003035]\n",
            "249 [D loss: 0.621362, acc.: 67.19%] [G loss: 1.095564]\n",
            "250 [D loss: 0.654018, acc.: 59.38%] [G loss: 1.047247]\n",
            "251 [D loss: 0.645319, acc.: 60.94%] [G loss: 1.014109]\n",
            "252 [D loss: 0.611824, acc.: 68.75%] [G loss: 0.980750]\n",
            "253 [D loss: 0.621991, acc.: 68.75%] [G loss: 1.014579]\n",
            "254 [D loss: 0.668215, acc.: 59.38%] [G loss: 0.969280]\n",
            "255 [D loss: 0.771491, acc.: 45.31%] [G loss: 0.826253]\n",
            "256 [D loss: 0.664958, acc.: 60.94%] [G loss: 0.788381]\n",
            "257 [D loss: 0.674385, acc.: 60.94%] [G loss: 0.789612]\n",
            "258 [D loss: 0.660379, acc.: 67.19%] [G loss: 0.799973]\n",
            "259 [D loss: 0.662859, acc.: 59.38%] [G loss: 0.815811]\n",
            "260 [D loss: 0.662788, acc.: 60.94%] [G loss: 0.804457]\n",
            "261 [D loss: 0.683101, acc.: 56.25%] [G loss: 0.790821]\n",
            "262 [D loss: 0.673475, acc.: 54.69%] [G loss: 0.811413]\n",
            "263 [D loss: 0.611800, acc.: 65.62%] [G loss: 0.851484]\n",
            "264 [D loss: 0.678272, acc.: 57.81%] [G loss: 0.827997]\n",
            "265 [D loss: 0.648282, acc.: 64.06%] [G loss: 0.837401]\n",
            "266 [D loss: 0.631979, acc.: 68.75%] [G loss: 0.859805]\n",
            "267 [D loss: 0.711970, acc.: 56.25%] [G loss: 0.816641]\n",
            "268 [D loss: 0.618133, acc.: 71.88%] [G loss: 0.819170]\n",
            "269 [D loss: 0.621360, acc.: 68.75%] [G loss: 0.839749]\n",
            "270 [D loss: 0.622029, acc.: 70.31%] [G loss: 0.849021]\n",
            "271 [D loss: 0.727532, acc.: 46.88%] [G loss: 0.860969]\n",
            "272 [D loss: 0.671926, acc.: 57.81%] [G loss: 0.875181]\n",
            "273 [D loss: 0.650054, acc.: 71.88%] [G loss: 0.817092]\n",
            "274 [D loss: 0.655514, acc.: 60.94%] [G loss: 0.821126]\n",
            "275 [D loss: 0.671731, acc.: 60.94%] [G loss: 0.812908]\n",
            "276 [D loss: 0.639832, acc.: 62.50%] [G loss: 0.847211]\n",
            "277 [D loss: 0.674234, acc.: 60.94%] [G loss: 0.861019]\n",
            "278 [D loss: 0.706722, acc.: 50.00%] [G loss: 0.828880]\n",
            "279 [D loss: 0.688482, acc.: 60.94%] [G loss: 0.827390]\n",
            "280 [D loss: 0.622542, acc.: 71.88%] [G loss: 0.860628]\n",
            "281 [D loss: 0.663670, acc.: 57.81%] [G loss: 0.852794]\n",
            "282 [D loss: 0.645814, acc.: 62.50%] [G loss: 0.868131]\n",
            "283 [D loss: 0.683448, acc.: 56.25%] [G loss: 0.910758]\n",
            "284 [D loss: 0.663415, acc.: 59.38%] [G loss: 0.825183]\n",
            "285 [D loss: 0.679196, acc.: 65.62%] [G loss: 0.868483]\n",
            "286 [D loss: 0.608338, acc.: 76.56%] [G loss: 1.160293]\n",
            "287 [D loss: 0.896595, acc.: 34.38%] [G loss: 0.852379]\n",
            "288 [D loss: 0.711380, acc.: 45.31%] [G loss: 0.827577]\n",
            "289 [D loss: 0.728526, acc.: 43.75%] [G loss: 0.783836]\n",
            "290 [D loss: 0.694918, acc.: 62.50%] [G loss: 0.767346]\n",
            "291 [D loss: 0.705224, acc.: 48.44%] [G loss: 0.769027]\n",
            "292 [D loss: 0.711895, acc.: 48.44%] [G loss: 0.753517]\n",
            "293 [D loss: 0.704093, acc.: 54.69%] [G loss: 0.746004]\n",
            "294 [D loss: 0.703385, acc.: 48.44%] [G loss: 0.741348]\n",
            "295 [D loss: 0.699262, acc.: 59.38%] [G loss: 0.745520]\n",
            "296 [D loss: 0.688266, acc.: 59.38%] [G loss: 0.741789]\n",
            "297 [D loss: 0.691686, acc.: 56.25%] [G loss: 0.742601]\n",
            "298 [D loss: 0.684648, acc.: 64.06%] [G loss: 0.746228]\n",
            "299 [D loss: 0.684373, acc.: 65.62%] [G loss: 0.751628]\n",
            "300 [D loss: 0.683783, acc.: 54.69%] [G loss: 0.761895]\n",
            "301 [D loss: 0.693765, acc.: 54.69%] [G loss: 0.755450]\n",
            "302 [D loss: 0.687187, acc.: 56.25%] [G loss: 0.751729]\n",
            "303 [D loss: 0.686024, acc.: 60.94%] [G loss: 0.762805]\n",
            "304 [D loss: 0.688903, acc.: 60.94%] [G loss: 0.755477]\n",
            "305 [D loss: 0.706340, acc.: 57.81%] [G loss: 0.758608]\n",
            "306 [D loss: 0.688216, acc.: 57.81%] [G loss: 0.752767]\n",
            "307 [D loss: 0.666124, acc.: 60.94%] [G loss: 0.771086]\n",
            "308 [D loss: 0.691414, acc.: 51.56%] [G loss: 0.769036]\n",
            "309 [D loss: 0.727962, acc.: 45.31%] [G loss: 0.747122]\n",
            "310 [D loss: 0.690134, acc.: 59.38%] [G loss: 0.730439]\n",
            "311 [D loss: 0.683301, acc.: 56.25%] [G loss: 0.737624]\n",
            "312 [D loss: 0.695986, acc.: 51.56%] [G loss: 0.722306]\n",
            "313 [D loss: 0.686140, acc.: 57.81%] [G loss: 0.733130]\n",
            "314 [D loss: 0.691883, acc.: 51.56%] [G loss: 0.710622]\n",
            "315 [D loss: 0.693435, acc.: 53.12%] [G loss: 0.732741]\n",
            "316 [D loss: 0.688190, acc.: 56.25%] [G loss: 0.721623]\n",
            "317 [D loss: 0.699776, acc.: 57.81%] [G loss: 0.746041]\n",
            "318 [D loss: 0.690333, acc.: 59.38%] [G loss: 0.732199]\n",
            "319 [D loss: 0.690013, acc.: 53.12%] [G loss: 0.727967]\n",
            "320 [D loss: 0.684019, acc.: 54.69%] [G loss: 0.739254]\n",
            "321 [D loss: 0.683459, acc.: 67.19%] [G loss: 0.742143]\n",
            "322 [D loss: 0.687001, acc.: 60.94%] [G loss: 0.726065]\n",
            "323 [D loss: 0.676339, acc.: 64.06%] [G loss: 0.748815]\n",
            "324 [D loss: 0.695762, acc.: 56.25%] [G loss: 0.750784]\n",
            "325 [D loss: 0.694710, acc.: 51.56%] [G loss: 0.741660]\n",
            "326 [D loss: 0.705083, acc.: 56.25%] [G loss: 0.736451]\n",
            "327 [D loss: 0.691114, acc.: 53.12%] [G loss: 0.736816]\n",
            "328 [D loss: 0.688838, acc.: 57.81%] [G loss: 0.720735]\n",
            "329 [D loss: 0.682852, acc.: 60.94%] [G loss: 0.736028]\n",
            "330 [D loss: 0.696626, acc.: 57.81%] [G loss: 0.724490]\n",
            "331 [D loss: 0.698267, acc.: 46.88%] [G loss: 0.725829]\n",
            "332 [D loss: 0.678627, acc.: 65.62%] [G loss: 0.734447]\n",
            "333 [D loss: 0.688612, acc.: 50.00%] [G loss: 0.731668]\n",
            "334 [D loss: 0.696698, acc.: 46.88%] [G loss: 0.720768]\n",
            "335 [D loss: 0.695769, acc.: 48.44%] [G loss: 0.747744]\n",
            "336 [D loss: 0.687800, acc.: 48.44%] [G loss: 0.742526]\n",
            "337 [D loss: 0.685086, acc.: 59.38%] [G loss: 0.750940]\n",
            "338 [D loss: 0.683605, acc.: 57.81%] [G loss: 0.747797]\n",
            "339 [D loss: 0.684267, acc.: 54.69%] [G loss: 0.747119]\n",
            "340 [D loss: 0.691618, acc.: 59.38%] [G loss: 0.737453]\n",
            "341 [D loss: 0.701460, acc.: 51.56%] [G loss: 0.739274]\n",
            "342 [D loss: 0.681141, acc.: 65.62%] [G loss: 0.732295]\n",
            "343 [D loss: 0.693981, acc.: 48.44%] [G loss: 0.733921]\n",
            "344 [D loss: 0.679857, acc.: 60.94%] [G loss: 0.731734]\n",
            "345 [D loss: 0.680542, acc.: 64.06%] [G loss: 0.731991]\n",
            "346 [D loss: 0.696805, acc.: 56.25%] [G loss: 0.724057]\n",
            "347 [D loss: 0.694978, acc.: 50.00%] [G loss: 0.717059]\n",
            "348 [D loss: 0.695765, acc.: 53.12%] [G loss: 0.724825]\n",
            "349 [D loss: 0.685166, acc.: 57.81%] [G loss: 0.729574]\n",
            "350 [D loss: 0.703234, acc.: 54.69%] [G loss: 0.723126]\n",
            "351 [D loss: 0.681798, acc.: 53.12%] [G loss: 0.712055]\n",
            "352 [D loss: 0.688949, acc.: 54.69%] [G loss: 0.719926]\n",
            "353 [D loss: 0.679975, acc.: 60.94%] [G loss: 0.726568]\n",
            "354 [D loss: 0.685296, acc.: 53.12%] [G loss: 0.722370]\n",
            "355 [D loss: 0.670585, acc.: 59.38%] [G loss: 0.739738]\n",
            "356 [D loss: 0.685367, acc.: 59.38%] [G loss: 0.730908]\n",
            "357 [D loss: 0.678896, acc.: 59.38%] [G loss: 0.738888]\n",
            "358 [D loss: 0.704784, acc.: 51.56%] [G loss: 0.729530]\n",
            "359 [D loss: 0.685169, acc.: 60.94%] [G loss: 0.731535]\n",
            "360 [D loss: 0.684271, acc.: 57.81%] [G loss: 0.729305]\n",
            "361 [D loss: 0.688484, acc.: 45.31%] [G loss: 0.743134]\n",
            "362 [D loss: 0.682410, acc.: 62.50%] [G loss: 0.723633]\n",
            "363 [D loss: 0.680209, acc.: 53.12%] [G loss: 0.724840]\n",
            "364 [D loss: 0.691575, acc.: 57.81%] [G loss: 0.740958]\n",
            "365 [D loss: 0.706553, acc.: 50.00%] [G loss: 0.742163]\n",
            "366 [D loss: 0.700612, acc.: 48.44%] [G loss: 0.739910]\n",
            "367 [D loss: 0.691035, acc.: 53.12%] [G loss: 0.725413]\n",
            "368 [D loss: 0.686789, acc.: 57.81%] [G loss: 0.740359]\n",
            "369 [D loss: 0.683899, acc.: 62.50%] [G loss: 0.738239]\n",
            "370 [D loss: 0.688715, acc.: 59.38%] [G loss: 0.717444]\n",
            "371 [D loss: 0.693087, acc.: 57.81%] [G loss: 0.722331]\n",
            "372 [D loss: 0.692493, acc.: 57.81%] [G loss: 0.721433]\n",
            "373 [D loss: 0.686070, acc.: 65.62%] [G loss: 0.708672]\n",
            "374 [D loss: 0.689635, acc.: 53.12%] [G loss: 0.719049]\n",
            "375 [D loss: 0.690413, acc.: 60.94%] [G loss: 0.711965]\n",
            "376 [D loss: 0.687818, acc.: 56.25%] [G loss: 0.702983]\n",
            "377 [D loss: 0.678925, acc.: 67.19%] [G loss: 0.712058]\n",
            "378 [D loss: 0.693330, acc.: 48.44%] [G loss: 0.703542]\n",
            "379 [D loss: 0.700406, acc.: 50.00%] [G loss: 0.716087]\n",
            "380 [D loss: 0.682639, acc.: 56.25%] [G loss: 0.705889]\n",
            "381 [D loss: 0.695765, acc.: 45.31%] [G loss: 0.718437]\n",
            "382 [D loss: 0.680991, acc.: 64.06%] [G loss: 0.711561]\n",
            "383 [D loss: 0.687093, acc.: 62.50%] [G loss: 0.727605]\n",
            "384 [D loss: 0.690692, acc.: 51.56%] [G loss: 0.727128]\n",
            "385 [D loss: 0.688905, acc.: 56.25%] [G loss: 0.705730]\n",
            "386 [D loss: 0.696550, acc.: 43.75%] [G loss: 0.715809]\n",
            "387 [D loss: 0.687237, acc.: 54.69%] [G loss: 0.711477]\n",
            "388 [D loss: 0.694718, acc.: 53.12%] [G loss: 0.724794]\n",
            "389 [D loss: 0.678887, acc.: 67.19%] [G loss: 0.731577]\n",
            "390 [D loss: 0.685708, acc.: 53.12%] [G loss: 0.733207]\n",
            "391 [D loss: 0.694911, acc.: 54.69%] [G loss: 0.737756]\n",
            "392 [D loss: 0.680292, acc.: 68.75%] [G loss: 0.740751]\n",
            "393 [D loss: 0.687294, acc.: 57.81%] [G loss: 0.728143]\n",
            "394 [D loss: 0.697704, acc.: 46.88%] [G loss: 0.718245]\n",
            "395 [D loss: 0.688236, acc.: 51.56%] [G loss: 0.709701]\n",
            "396 [D loss: 0.678812, acc.: 60.94%] [G loss: 0.730449]\n",
            "397 [D loss: 0.690006, acc.: 51.56%] [G loss: 0.723015]\n",
            "398 [D loss: 0.695002, acc.: 54.69%] [G loss: 0.708274]\n",
            "399 [D loss: 0.681333, acc.: 70.31%] [G loss: 0.720065]\n",
            "400 [D loss: 0.695151, acc.: 39.06%] [G loss: 0.718902]\n",
            "401 [D loss: 0.674766, acc.: 70.31%] [G loss: 0.717744]\n",
            "402 [D loss: 0.680987, acc.: 60.94%] [G loss: 0.711309]\n",
            "403 [D loss: 0.687067, acc.: 53.12%] [G loss: 0.703772]\n",
            "404 [D loss: 0.688348, acc.: 54.69%] [G loss: 0.724832]\n",
            "405 [D loss: 0.674979, acc.: 64.06%] [G loss: 0.733323]\n",
            "406 [D loss: 0.684315, acc.: 53.12%] [G loss: 0.743338]\n",
            "407 [D loss: 0.708031, acc.: 42.19%] [G loss: 0.737595]\n",
            "408 [D loss: 0.683040, acc.: 68.75%] [G loss: 0.737311]\n",
            "409 [D loss: 0.687097, acc.: 60.94%] [G loss: 0.748710]\n",
            "410 [D loss: 0.697126, acc.: 50.00%] [G loss: 0.737008]\n",
            "411 [D loss: 0.683004, acc.: 62.50%] [G loss: 0.747008]\n",
            "412 [D loss: 0.679193, acc.: 64.06%] [G loss: 0.738406]\n",
            "413 [D loss: 0.675389, acc.: 70.31%] [G loss: 0.744791]\n",
            "414 [D loss: 0.705149, acc.: 57.81%] [G loss: 0.744942]\n",
            "415 [D loss: 0.675453, acc.: 64.06%] [G loss: 0.749114]\n",
            "416 [D loss: 0.703035, acc.: 57.81%] [G loss: 0.715327]\n",
            "417 [D loss: 0.689540, acc.: 57.81%] [G loss: 0.736820]\n",
            "418 [D loss: 0.685434, acc.: 57.81%] [G loss: 0.730001]\n",
            "419 [D loss: 0.684780, acc.: 60.94%] [G loss: 0.742551]\n",
            "420 [D loss: 0.683106, acc.: 57.81%] [G loss: 0.761722]\n",
            "421 [D loss: 0.670488, acc.: 65.62%] [G loss: 0.739704]\n",
            "422 [D loss: 0.688908, acc.: 57.81%] [G loss: 0.763715]\n",
            "423 [D loss: 0.680980, acc.: 57.81%] [G loss: 0.727720]\n",
            "424 [D loss: 0.692173, acc.: 54.69%] [G loss: 0.739118]\n",
            "425 [D loss: 0.674207, acc.: 60.94%] [G loss: 0.720111]\n",
            "426 [D loss: 0.667473, acc.: 65.62%] [G loss: 0.732913]\n",
            "427 [D loss: 0.668958, acc.: 67.19%] [G loss: 0.739898]\n",
            "428 [D loss: 0.694947, acc.: 48.44%] [G loss: 0.749164]\n",
            "429 [D loss: 0.706399, acc.: 46.88%] [G loss: 0.739314]\n",
            "430 [D loss: 0.665045, acc.: 71.88%] [G loss: 0.727277]\n",
            "431 [D loss: 0.702897, acc.: 48.44%] [G loss: 0.740831]\n",
            "432 [D loss: 0.680449, acc.: 59.38%] [G loss: 0.741425]\n",
            "433 [D loss: 0.672903, acc.: 57.81%] [G loss: 0.744999]\n",
            "434 [D loss: 0.675501, acc.: 64.06%] [G loss: 0.766986]\n",
            "435 [D loss: 0.681249, acc.: 62.50%] [G loss: 0.775105]\n",
            "436 [D loss: 0.670308, acc.: 60.94%] [G loss: 0.808382]\n",
            "437 [D loss: 0.694031, acc.: 56.25%] [G loss: 0.806083]\n",
            "438 [D loss: 0.685802, acc.: 57.81%] [G loss: 0.768116]\n",
            "439 [D loss: 0.708702, acc.: 50.00%] [G loss: 0.754033]\n",
            "440 [D loss: 0.680758, acc.: 59.38%] [G loss: 0.769287]\n",
            "441 [D loss: 0.700156, acc.: 60.94%] [G loss: 0.747848]\n",
            "442 [D loss: 0.689515, acc.: 62.50%] [G loss: 0.761684]\n",
            "443 [D loss: 0.674966, acc.: 65.62%] [G loss: 0.759812]\n",
            "444 [D loss: 0.666120, acc.: 62.50%] [G loss: 0.776335]\n",
            "445 [D loss: 0.687460, acc.: 56.25%] [G loss: 0.781469]\n",
            "446 [D loss: 0.690221, acc.: 62.50%] [G loss: 0.759041]\n",
            "447 [D loss: 0.664579, acc.: 67.19%] [G loss: 0.744443]\n",
            "448 [D loss: 0.701219, acc.: 56.25%] [G loss: 0.757134]\n",
            "449 [D loss: 0.655410, acc.: 71.88%] [G loss: 0.737620]\n",
            "450 [D loss: 0.697465, acc.: 54.69%] [G loss: 0.727839]\n",
            "451 [D loss: 0.679697, acc.: 60.94%] [G loss: 0.749142]\n",
            "452 [D loss: 0.711802, acc.: 50.00%] [G loss: 0.743193]\n",
            "453 [D loss: 0.658521, acc.: 71.88%] [G loss: 0.714936]\n",
            "454 [D loss: 0.675551, acc.: 59.38%] [G loss: 0.814057]\n",
            "455 [D loss: 0.736992, acc.: 37.50%] [G loss: 0.762368]\n",
            "456 [D loss: 0.699637, acc.: 54.69%] [G loss: 0.718089]\n",
            "457 [D loss: 0.674017, acc.: 65.62%] [G loss: 0.741835]\n",
            "458 [D loss: 0.668203, acc.: 62.50%] [G loss: 0.730433]\n",
            "459 [D loss: 0.663526, acc.: 64.06%] [G loss: 0.799148]\n",
            "460 [D loss: 0.691107, acc.: 50.00%] [G loss: 0.739861]\n",
            "461 [D loss: 0.681550, acc.: 62.50%] [G loss: 0.751295]\n",
            "462 [D loss: 0.676039, acc.: 60.94%] [G loss: 0.803330]\n",
            "463 [D loss: 0.706717, acc.: 46.88%] [G loss: 0.728015]\n",
            "464 [D loss: 0.682926, acc.: 62.50%] [G loss: 0.781791]\n",
            "465 [D loss: 0.690590, acc.: 54.69%] [G loss: 0.735216]\n",
            "466 [D loss: 0.699594, acc.: 45.31%] [G loss: 0.727243]\n",
            "467 [D loss: 0.676595, acc.: 65.62%] [G loss: 0.740746]\n",
            "468 [D loss: 0.695751, acc.: 46.88%] [G loss: 0.721390]\n",
            "469 [D loss: 0.669827, acc.: 59.38%] [G loss: 0.748266]\n",
            "470 [D loss: 0.687597, acc.: 54.69%] [G loss: 0.762993]\n",
            "471 [D loss: 0.707095, acc.: 48.44%] [G loss: 0.731213]\n",
            "472 [D loss: 0.670601, acc.: 67.19%] [G loss: 0.732822]\n",
            "473 [D loss: 0.663657, acc.: 68.75%] [G loss: 0.741592]\n",
            "474 [D loss: 0.693262, acc.: 54.69%] [G loss: 0.723813]\n",
            "475 [D loss: 0.662077, acc.: 64.06%] [G loss: 0.750862]\n",
            "476 [D loss: 0.683451, acc.: 64.06%] [G loss: 0.765643]\n",
            "477 [D loss: 0.670114, acc.: 65.62%] [G loss: 0.764483]\n",
            "478 [D loss: 0.670359, acc.: 60.94%] [G loss: 0.737174]\n",
            "479 [D loss: 0.670051, acc.: 62.50%] [G loss: 0.762826]\n",
            "480 [D loss: 0.651093, acc.: 68.75%] [G loss: 0.737488]\n",
            "481 [D loss: 0.677038, acc.: 57.81%] [G loss: 0.766089]\n",
            "482 [D loss: 0.692409, acc.: 57.81%] [G loss: 0.804483]\n",
            "483 [D loss: 0.705174, acc.: 50.00%] [G loss: 0.740096]\n",
            "484 [D loss: 0.671654, acc.: 64.06%] [G loss: 0.803836]\n",
            "485 [D loss: 0.661062, acc.: 70.31%] [G loss: 0.785329]\n",
            "486 [D loss: 0.654413, acc.: 64.06%] [G loss: 0.771269]\n",
            "487 [D loss: 0.672466, acc.: 59.38%] [G loss: 0.792349]\n",
            "488 [D loss: 0.673574, acc.: 65.62%] [G loss: 0.794040]\n",
            "489 [D loss: 0.652571, acc.: 60.94%] [G loss: 0.811473]\n",
            "490 [D loss: 0.669102, acc.: 60.94%] [G loss: 0.812273]\n",
            "491 [D loss: 0.670376, acc.: 59.38%] [G loss: 0.797040]\n",
            "492 [D loss: 0.718488, acc.: 48.44%] [G loss: 0.769941]\n",
            "493 [D loss: 0.663924, acc.: 68.75%] [G loss: 0.848468]\n",
            "494 [D loss: 0.668699, acc.: 57.81%] [G loss: 0.763455]\n",
            "495 [D loss: 0.684346, acc.: 62.50%] [G loss: 0.794209]\n",
            "496 [D loss: 0.682582, acc.: 60.94%] [G loss: 0.798545]\n",
            "497 [D loss: 0.655696, acc.: 65.62%] [G loss: 0.832613]\n",
            "498 [D loss: 0.687133, acc.: 54.69%] [G loss: 0.760125]\n",
            "499 [D loss: 0.709908, acc.: 48.44%] [G loss: 0.727325]\n",
            "500 [D loss: 0.653523, acc.: 68.75%] [G loss: 0.849253]\n",
            "501 [D loss: 0.692101, acc.: 56.25%] [G loss: 0.831848]\n",
            "502 [D loss: 0.706148, acc.: 43.75%] [G loss: 0.801676]\n",
            "503 [D loss: 0.660236, acc.: 68.75%] [G loss: 0.847530]\n",
            "504 [D loss: 0.691196, acc.: 57.81%] [G loss: 0.832613]\n",
            "505 [D loss: 0.673362, acc.: 54.69%] [G loss: 0.824723]\n",
            "506 [D loss: 0.669338, acc.: 60.94%] [G loss: 0.858167]\n",
            "507 [D loss: 0.676690, acc.: 56.25%] [G loss: 0.816694]\n",
            "508 [D loss: 0.630871, acc.: 71.88%] [G loss: 0.826093]\n",
            "509 [D loss: 0.649704, acc.: 68.75%] [G loss: 0.867484]\n",
            "510 [D loss: 0.672486, acc.: 57.81%] [G loss: 0.783110]\n",
            "511 [D loss: 0.692377, acc.: 60.94%] [G loss: 0.784470]\n",
            "512 [D loss: 0.652069, acc.: 67.19%] [G loss: 0.862144]\n",
            "513 [D loss: 0.684747, acc.: 57.81%] [G loss: 0.810702]\n",
            "514 [D loss: 0.655960, acc.: 67.19%] [G loss: 0.823908]\n",
            "515 [D loss: 0.680640, acc.: 62.50%] [G loss: 0.813532]\n",
            "516 [D loss: 0.648953, acc.: 67.19%] [G loss: 0.848649]\n",
            "517 [D loss: 0.659310, acc.: 59.38%] [G loss: 0.788668]\n",
            "518 [D loss: 0.635851, acc.: 68.75%] [G loss: 0.894972]\n",
            "519 [D loss: 0.691370, acc.: 57.81%] [G loss: 0.799926]\n",
            "520 [D loss: 0.703330, acc.: 54.69%] [G loss: 0.779921]\n",
            "521 [D loss: 0.654339, acc.: 64.06%] [G loss: 0.862788]\n",
            "522 [D loss: 0.666963, acc.: 64.06%] [G loss: 0.826612]\n",
            "523 [D loss: 0.676082, acc.: 59.38%] [G loss: 0.880553]\n",
            "524 [D loss: 0.678904, acc.: 54.69%] [G loss: 0.789004]\n",
            "525 [D loss: 0.653239, acc.: 65.62%] [G loss: 0.793849]\n",
            "526 [D loss: 0.648205, acc.: 65.62%] [G loss: 0.860168]\n",
            "527 [D loss: 0.671966, acc.: 68.75%] [G loss: 0.943234]\n",
            "528 [D loss: 0.666883, acc.: 62.50%] [G loss: 0.824718]\n",
            "529 [D loss: 0.643011, acc.: 65.62%] [G loss: 0.843764]\n",
            "530 [D loss: 0.682366, acc.: 64.06%] [G loss: 0.906962]\n",
            "531 [D loss: 0.662460, acc.: 64.06%] [G loss: 0.932559]\n",
            "532 [D loss: 0.667910, acc.: 70.31%] [G loss: 0.846727]\n",
            "533 [D loss: 0.624168, acc.: 70.31%] [G loss: 0.869833]\n",
            "534 [D loss: 0.638075, acc.: 68.75%] [G loss: 0.979417]\n",
            "535 [D loss: 0.690368, acc.: 57.81%] [G loss: 0.865300]\n",
            "536 [D loss: 0.683639, acc.: 56.25%] [G loss: 0.841396]\n",
            "537 [D loss: 0.631987, acc.: 64.06%] [G loss: 0.849388]\n",
            "538 [D loss: 0.697602, acc.: 65.62%] [G loss: 1.021369]\n",
            "539 [D loss: 0.641507, acc.: 67.19%] [G loss: 0.768763]\n",
            "540 [D loss: 0.615642, acc.: 73.44%] [G loss: 0.882096]\n",
            "541 [D loss: 0.645845, acc.: 68.75%] [G loss: 0.873133]\n",
            "542 [D loss: 0.657178, acc.: 64.06%] [G loss: 0.964398]\n",
            "543 [D loss: 0.681596, acc.: 60.94%] [G loss: 0.861052]\n",
            "544 [D loss: 0.617215, acc.: 73.44%] [G loss: 0.880456]\n",
            "545 [D loss: 0.652775, acc.: 67.19%] [G loss: 0.889387]\n",
            "546 [D loss: 0.655057, acc.: 62.50%] [G loss: 0.908384]\n",
            "547 [D loss: 0.653538, acc.: 60.94%] [G loss: 0.874291]\n",
            "548 [D loss: 0.578627, acc.: 76.56%] [G loss: 1.007303]\n",
            "549 [D loss: 0.690967, acc.: 59.38%] [G loss: 1.032541]\n",
            "550 [D loss: 0.671215, acc.: 64.06%] [G loss: 0.861551]\n",
            "551 [D loss: 0.671232, acc.: 64.06%] [G loss: 0.940674]\n",
            "552 [D loss: 0.627580, acc.: 71.88%] [G loss: 0.828938]\n",
            "553 [D loss: 0.625312, acc.: 67.19%] [G loss: 0.859122]\n",
            "554 [D loss: 0.599506, acc.: 75.00%] [G loss: 0.883359]\n",
            "555 [D loss: 0.610633, acc.: 70.31%] [G loss: 1.012683]\n",
            "556 [D loss: 0.598323, acc.: 71.88%] [G loss: 1.025533]\n",
            "557 [D loss: 0.663231, acc.: 70.31%] [G loss: 0.976208]\n",
            "558 [D loss: 0.678951, acc.: 57.81%] [G loss: 0.843302]\n",
            "559 [D loss: 0.677231, acc.: 62.50%] [G loss: 0.842237]\n",
            "560 [D loss: 0.667898, acc.: 59.38%] [G loss: 0.958688]\n",
            "561 [D loss: 0.644107, acc.: 68.75%] [G loss: 0.854684]\n",
            "562 [D loss: 0.683452, acc.: 56.25%] [G loss: 0.902329]\n",
            "563 [D loss: 0.597707, acc.: 70.31%] [G loss: 0.976646]\n",
            "564 [D loss: 0.733906, acc.: 57.81%] [G loss: 0.869651]\n",
            "565 [D loss: 0.669326, acc.: 65.62%] [G loss: 0.778359]\n",
            "566 [D loss: 0.627183, acc.: 64.06%] [G loss: 0.890204]\n",
            "567 [D loss: 0.617651, acc.: 68.75%] [G loss: 0.890914]\n",
            "568 [D loss: 0.640182, acc.: 70.31%] [G loss: 0.872918]\n",
            "569 [D loss: 0.659277, acc.: 64.06%] [G loss: 0.949519]\n",
            "570 [D loss: 0.645485, acc.: 57.81%] [G loss: 1.007534]\n",
            "571 [D loss: 0.631550, acc.: 59.38%] [G loss: 1.282781]\n",
            "572 [D loss: 0.677969, acc.: 59.38%] [G loss: 0.848213]\n",
            "573 [D loss: 0.645807, acc.: 68.75%] [G loss: 0.938118]\n",
            "574 [D loss: 0.623499, acc.: 62.50%] [G loss: 1.000878]\n",
            "575 [D loss: 0.679579, acc.: 70.31%] [G loss: 2.179685]\n",
            "576 [D loss: 1.497817, acc.: 45.31%] [G loss: 1.338950]\n",
            "577 [D loss: 0.753888, acc.: 56.25%] [G loss: 1.129606]\n",
            "578 [D loss: 0.668628, acc.: 56.25%] [G loss: 1.053970]\n",
            "579 [D loss: 0.689309, acc.: 50.00%] [G loss: 1.066516]\n",
            "580 [D loss: 0.719557, acc.: 53.12%] [G loss: 0.970696]\n",
            "581 [D loss: 0.670822, acc.: 64.06%] [G loss: 0.921167]\n",
            "582 [D loss: 0.629704, acc.: 65.62%] [G loss: 0.896902]\n",
            "583 [D loss: 0.669376, acc.: 65.62%] [G loss: 0.959303]\n",
            "584 [D loss: 0.631478, acc.: 70.31%] [G loss: 0.873681]\n",
            "585 [D loss: 0.642238, acc.: 64.06%] [G loss: 0.891834]\n",
            "586 [D loss: 0.636191, acc.: 67.19%] [G loss: 0.951536]\n",
            "587 [D loss: 0.622428, acc.: 67.19%] [G loss: 0.979264]\n",
            "588 [D loss: 0.627187, acc.: 68.75%] [G loss: 0.904146]\n",
            "589 [D loss: 0.600368, acc.: 68.75%] [G loss: 0.935608]\n",
            "590 [D loss: 0.624114, acc.: 73.44%] [G loss: 1.035824]\n",
            "591 [D loss: 0.689900, acc.: 54.69%] [G loss: 0.986609]\n",
            "592 [D loss: 0.618285, acc.: 71.88%] [G loss: 0.968488]\n",
            "593 [D loss: 0.562782, acc.: 75.00%] [G loss: 0.869462]\n",
            "594 [D loss: 0.638512, acc.: 62.50%] [G loss: 0.963956]\n",
            "595 [D loss: 0.604753, acc.: 71.88%] [G loss: 0.933888]\n",
            "596 [D loss: 0.586023, acc.: 70.31%] [G loss: 0.989349]\n",
            "597 [D loss: 0.699097, acc.: 56.25%] [G loss: 0.941766]\n",
            "598 [D loss: 0.613620, acc.: 68.75%] [G loss: 0.966993]\n",
            "599 [D loss: 0.683096, acc.: 60.94%] [G loss: 0.968758]\n",
            "600 [D loss: 0.662453, acc.: 64.06%] [G loss: 0.891504]\n",
            "601 [D loss: 0.634855, acc.: 68.75%] [G loss: 0.888620]\n",
            "602 [D loss: 0.647832, acc.: 65.62%] [G loss: 0.848458]\n",
            "603 [D loss: 0.681197, acc.: 59.38%] [G loss: 0.894207]\n",
            "604 [D loss: 0.642421, acc.: 65.62%] [G loss: 0.869297]\n",
            "605 [D loss: 0.564403, acc.: 79.69%] [G loss: 0.882944]\n",
            "606 [D loss: 0.640003, acc.: 67.19%] [G loss: 0.930572]\n",
            "607 [D loss: 0.626294, acc.: 67.19%] [G loss: 0.936920]\n",
            "608 [D loss: 0.622291, acc.: 68.75%] [G loss: 0.924043]\n",
            "609 [D loss: 0.611477, acc.: 70.31%] [G loss: 0.844904]\n",
            "610 [D loss: 0.627064, acc.: 62.50%] [G loss: 0.929186]\n",
            "611 [D loss: 0.705108, acc.: 54.69%] [G loss: 0.971474]\n",
            "612 [D loss: 0.572077, acc.: 75.00%] [G loss: 1.065444]\n",
            "613 [D loss: 0.631274, acc.: 60.94%] [G loss: 0.845711]\n",
            "614 [D loss: 0.598063, acc.: 68.75%] [G loss: 0.978273]\n",
            "615 [D loss: 0.574857, acc.: 71.88%] [G loss: 1.002546]\n",
            "616 [D loss: 0.562437, acc.: 75.00%] [G loss: 1.082544]\n",
            "617 [D loss: 0.689038, acc.: 56.25%] [G loss: 0.995467]\n",
            "618 [D loss: 0.582087, acc.: 73.44%] [G loss: 0.988990]\n",
            "619 [D loss: 0.648767, acc.: 71.88%] [G loss: 0.898098]\n",
            "620 [D loss: 0.587364, acc.: 75.00%] [G loss: 1.007217]\n",
            "621 [D loss: 0.604581, acc.: 70.31%] [G loss: 1.022045]\n",
            "622 [D loss: 0.601883, acc.: 73.44%] [G loss: 0.967986]\n",
            "623 [D loss: 0.533795, acc.: 76.56%] [G loss: 1.005451]\n",
            "624 [D loss: 0.610453, acc.: 60.94%] [G loss: 1.053342]\n",
            "625 [D loss: 0.644508, acc.: 65.62%] [G loss: 1.068058]\n",
            "626 [D loss: 0.589449, acc.: 67.19%] [G loss: 1.058411]\n",
            "627 [D loss: 0.564525, acc.: 73.44%] [G loss: 1.074673]\n",
            "628 [D loss: 0.665556, acc.: 62.50%] [G loss: 0.994330]\n",
            "629 [D loss: 0.669742, acc.: 59.38%] [G loss: 0.931574]\n",
            "630 [D loss: 0.613437, acc.: 71.88%] [G loss: 1.015543]\n",
            "631 [D loss: 0.569089, acc.: 71.88%] [G loss: 1.067411]\n",
            "632 [D loss: 0.985934, acc.: 54.69%] [G loss: 1.142498]\n",
            "633 [D loss: 0.623121, acc.: 67.19%] [G loss: 1.095091]\n",
            "634 [D loss: 0.687169, acc.: 56.25%] [G loss: 1.076499]\n",
            "635 [D loss: 0.606868, acc.: 70.31%] [G loss: 0.998337]\n",
            "636 [D loss: 0.574451, acc.: 76.56%] [G loss: 1.076052]\n",
            "637 [D loss: 0.633111, acc.: 70.31%] [G loss: 1.027009]\n",
            "638 [D loss: 0.583162, acc.: 75.00%] [G loss: 1.088117]\n",
            "639 [D loss: 0.653005, acc.: 60.94%] [G loss: 0.935441]\n",
            "640 [D loss: 0.556425, acc.: 75.00%] [G loss: 1.039086]\n",
            "641 [D loss: 0.523549, acc.: 79.69%] [G loss: 1.186400]\n",
            "642 [D loss: 0.595080, acc.: 73.44%] [G loss: 1.104239]\n",
            "643 [D loss: 0.606518, acc.: 70.31%] [G loss: 1.170454]\n",
            "644 [D loss: 0.533975, acc.: 78.12%] [G loss: 1.146782]\n",
            "645 [D loss: 0.532201, acc.: 73.44%] [G loss: 1.309895]\n",
            "646 [D loss: 0.653044, acc.: 62.50%] [G loss: 1.135568]\n",
            "647 [D loss: 0.596974, acc.: 73.44%] [G loss: 1.162036]\n",
            "648 [D loss: 0.553063, acc.: 68.75%] [G loss: 1.127047]\n",
            "649 [D loss: 0.610606, acc.: 67.19%] [G loss: 1.026566]\n",
            "650 [D loss: 0.552729, acc.: 71.88%] [G loss: 1.038140]\n",
            "651 [D loss: 0.506709, acc.: 76.56%] [G loss: 1.277365]\n",
            "652 [D loss: 0.572697, acc.: 68.75%] [G loss: 1.057071]\n",
            "653 [D loss: 0.507004, acc.: 79.69%] [G loss: 1.167776]\n",
            "654 [D loss: 0.642902, acc.: 70.31%] [G loss: 1.021903]\n",
            "655 [D loss: 0.686947, acc.: 60.94%] [G loss: 1.007839]\n",
            "656 [D loss: 0.543758, acc.: 78.12%] [G loss: 1.377630]\n",
            "657 [D loss: 0.600584, acc.: 70.31%] [G loss: 1.192588]\n",
            "658 [D loss: 0.583337, acc.: 71.88%] [G loss: 1.350752]\n",
            "659 [D loss: 0.403659, acc.: 85.94%] [G loss: 1.301851]\n",
            "660 [D loss: 0.621903, acc.: 67.19%] [G loss: 3.555448]\n",
            "661 [D loss: 2.288240, acc.: 46.88%] [G loss: 1.464977]\n",
            "662 [D loss: 0.649953, acc.: 67.19%] [G loss: 1.095476]\n",
            "663 [D loss: 0.652571, acc.: 62.50%] [G loss: 1.094904]\n",
            "664 [D loss: 0.492509, acc.: 70.31%] [G loss: 1.111084]\n",
            "665 [D loss: 0.535573, acc.: 75.00%] [G loss: 1.058753]\n",
            "666 [D loss: 0.624587, acc.: 68.75%] [G loss: 1.111385]\n",
            "667 [D loss: 0.488442, acc.: 79.69%] [G loss: 1.263615]\n",
            "668 [D loss: 0.460230, acc.: 75.00%] [G loss: 1.444828]\n",
            "669 [D loss: 0.580608, acc.: 75.00%] [G loss: 1.141708]\n",
            "670 [D loss: 0.584719, acc.: 68.75%] [G loss: 1.278689]\n",
            "671 [D loss: 0.521343, acc.: 76.56%] [G loss: 1.285295]\n",
            "672 [D loss: 1.078253, acc.: 48.44%] [G loss: 1.147932]\n",
            "673 [D loss: 0.681046, acc.: 59.38%] [G loss: 1.054312]\n",
            "674 [D loss: 0.760226, acc.: 53.12%] [G loss: 1.095360]\n",
            "675 [D loss: 0.751270, acc.: 45.31%] [G loss: 0.944151]\n",
            "676 [D loss: 0.737792, acc.: 54.69%] [G loss: 0.926724]\n",
            "677 [D loss: 0.593322, acc.: 71.88%] [G loss: 0.922918]\n",
            "678 [D loss: 0.577820, acc.: 71.88%] [G loss: 0.984066]\n",
            "679 [D loss: 0.526709, acc.: 75.00%] [G loss: 0.915581]\n",
            "680 [D loss: 0.795526, acc.: 56.25%] [G loss: 0.960310]\n",
            "681 [D loss: 0.531298, acc.: 68.75%] [G loss: 1.005014]\n",
            "682 [D loss: 0.614401, acc.: 73.44%] [G loss: 1.004841]\n",
            "683 [D loss: 0.563929, acc.: 70.31%] [G loss: 1.015736]\n",
            "684 [D loss: 0.525388, acc.: 76.56%] [G loss: 1.064749]\n",
            "685 [D loss: 0.560974, acc.: 68.75%] [G loss: 1.136253]\n",
            "686 [D loss: 0.579079, acc.: 67.19%] [G loss: 1.130295]\n",
            "687 [D loss: 0.468364, acc.: 79.69%] [G loss: 1.049856]\n",
            "688 [D loss: 0.555879, acc.: 71.88%] [G loss: 1.349878]\n",
            "689 [D loss: 0.526887, acc.: 73.44%] [G loss: 1.151758]\n",
            "690 [D loss: 0.677723, acc.: 65.62%] [G loss: 1.306227]\n",
            "691 [D loss: 0.685259, acc.: 59.38%] [G loss: 1.287112]\n",
            "692 [D loss: 0.563626, acc.: 71.88%] [G loss: 1.223882]\n",
            "693 [D loss: 0.536282, acc.: 79.69%] [G loss: 1.116731]\n",
            "694 [D loss: 0.573245, acc.: 67.19%] [G loss: 1.071096]\n",
            "695 [D loss: 0.488021, acc.: 79.69%] [G loss: 1.188004]\n",
            "696 [D loss: 0.540295, acc.: 76.56%] [G loss: 1.243390]\n",
            "697 [D loss: 0.554784, acc.: 73.44%] [G loss: 1.072263]\n",
            "698 [D loss: 0.540661, acc.: 71.88%] [G loss: 1.233906]\n",
            "699 [D loss: 0.563394, acc.: 71.88%] [G loss: 1.120106]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7n5ra4mI6yT",
        "colab_type": "code",
        "outputId": "a23d0ad7-bf16-43b2-a685-2f4e76d85934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "\"\"\"\n",
        "This will work sometimes and not work other times.\n",
        "There's an issue with the mapping of outputs to notes\n",
        "\"\"\"\n",
        "def generate(generator, latent_dim, input_notes):\n",
        "    # Get pitch names and store in a dictionary\n",
        "    notes = input_notes\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "    \n",
        "    # Use random noise to generate sequences\n",
        "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
        "    predictions = generator.predict(noise)\n",
        "    \n",
        "    n = len(pitchnames) / 2\n",
        "    pred_notes = [x*n+n for x in predictions[0]]\n",
        "    # pred_notes = [x*242+242 for x in predictions[0]]\n",
        "    pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
        "    \n",
        "    create_midi(pred_notes, 'gan_final')\n",
        "\n",
        "generate(gan,notes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fc3effb75a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcreate_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gan_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-fc3effb75a53>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(g, input_notes)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# pred_notes = [x*242+242 for x in predictions[0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_note\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcreate_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gan_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fc3effb75a53>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# pred_notes = [x*242+242 for x in predictions[0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_note\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcreate_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gan_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'int_to_note' is not defined"
          ]
        }
      ]
    }
  ]
}